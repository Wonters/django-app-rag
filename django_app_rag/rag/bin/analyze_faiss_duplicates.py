#!/usr/bin/env python3
"""
Script pour analyser les doublons dans l'index FAISS
et comprendre pourquoi il y a des chunks avec le mÃªme ID
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from django_app_rag.rag.retrievers import get_retriever
from django_app_rag.rag.models import Document
from collections import defaultdict
import hashlib

def analyze_faiss_duplicates():
    """Analyse les doublons dans l'index FAISS"""
    
    print("ğŸ” ANALYSE DES DOUBLONS DANS L'INDEX FAISS")
    print("=" * 80)
    
    # Initialiser le retriever
    retriever = get_retriever(
        embedding_model_id="sentence-transformers/all-MiniLM-L6-v2",
        embedding_model_type="huggingface",
        retriever_type="parent",
        k=3,
        device="cpu",
        vectorstore="faiss",
        persistent_path="media/rag_data/1",
    )
    
    # AccÃ©der au vectorstore
    vectorstore = retriever.vectorstore
    docstore = vectorstore.docstore
    
    print(f"ğŸ“Š Statistiques de l'index:")
    print(f"  - Total chunks: {len(docstore._dict)}")
    
    # Analyser les chunks par ID
    chunks_by_id = defaultdict(list)
    chunks_by_doc_id = defaultdict(list)
    chunks_by_content_hash = defaultdict(list)
    
    for chunk_id, chunk in docstore._dict.items():
        # Regroupement par ID de chunk
        chunks_by_id[chunk.metadata['id']].append(chunk)
        
        # Regroupement par ID de document
        chunks_by_doc_id[chunk.metadata['doc_id']].append(chunk)
        
        # Regroupement par hash du contenu
        content_hash = hashlib.md5(chunk.page_content.encode('utf-8')).hexdigest()
        chunks_by_content_hash[content_hash].append(chunk)
    
    print(f"  - Chunks uniques par ID: {len(chunks_by_id)}")
    print(f"  - Documents originaux: {len(chunks_by_doc_id)}")
    print(f"  - Contenus uniques: {len(chunks_by_content_hash)}")
    print()
    
    # === ANALYSE DES CHUNKS AVEC LE MÃŠME ID ===
    print("ğŸ” CHUNKS AVEC LE MÃŠME ID")
    print("-" * 50)
    
    id_duplicates = {chunk_id: chunks for chunk_id, chunks in chunks_by_id.items() if len(chunks) > 1}
    
    if id_duplicates:
        print(f"âš ï¸  PROBLÃˆME DÃ‰TECTÃ‰: {len(id_duplicates)} IDs de chunks dupliquÃ©s!")
        print()
        
        for chunk_id, chunks in id_duplicates.items():
            print(f"ğŸ“ ID dupliquÃ©: {chunk_id}")
            print(f"  Nombre de chunks: {len(chunks)}")
            print(f"  Chunks:")
            
            for i, chunk in enumerate(chunks):
                print(f"    Chunk {i+1}:")
                print(f"      - doc_id: {chunk.metadata['doc_id']}")
                print(f"      - title: {chunk.metadata.get('title', 'N/A')}")
                print(f"      - url: {chunk.metadata.get('url', 'N/A')}")
                print(f"      - content_hash: {hashlib.md5(chunk.page_content.encode('utf-8')).hexdigest()}")
                print(f"      - content_preview: {chunk.page_content[:100]}...")
                
                # VÃ©rifier si le contenu est identique
                if i > 0:
                    is_identical = chunk.page_content == chunks[0].page_content
                    print(f"      - Identique au premier: {'âœ… OUI' if is_identical else 'âŒ NON'}")
                print()
    else:
        print("âœ… Aucun doublon d'ID de chunk dÃ©tectÃ©")
    
    print()
    
    # === ANALYSE DES CHUNKS AVEC LE MÃŠME CONTENU ===
    print("ğŸ“„ CHUNKS AVEC LE MÃŠME CONTENU")
    print("-" * 50)
    
    content_duplicates = {content_hash: chunks for content_hash, chunks in chunks_by_content_hash.items() if len(chunks) > 1}
    
    if content_duplicates:
        print(f"ğŸ“Š {len(content_duplicates)} groupes de contenu dupliquÃ©:")
        print()
        
        for content_hash, chunks in content_duplicates.items():
            print(f"ğŸ” Hash de contenu: {content_hash}")
            print(f"  Nombre de chunks: {len(chunks)}")
            print(f"  AperÃ§u du contenu: {chunks[0].page_content[:100]}...")
            print(f"  Chunks:")
            
            for i, chunk in enumerate(chunks):
                print(f"    - Chunk {i+1}: ID={chunk.metadata['id']}, doc_id={chunk.metadata['doc_id']}")
                print(f"      Title: {chunk.metadata.get('title', 'N/A')}")
                print(f"      URL: {chunk.metadata.get('url', 'N/A')}")
            print()
    else:
        print("âœ… Aucun doublon de contenu dÃ©tectÃ©")
    
    print()
    
    # === ANALYSE DES DOCUMENTS AVEC PLUSIEURS CHUNKS ===
    print("ğŸ“š DOCUMENTS AVEC PLUSIEURS CHUNKS")
    print("-" * 50)
    
    doc_multiple_chunks = {doc_id: chunks for doc_id, chunks in chunks_by_doc_id.items() if len(chunks) > 1}
    
    if doc_multiple_chunks:
        print(f"ğŸ“Š {len(doc_multiple_chunks)} documents avec plusieurs chunks:")
        print()
        
        # Trier par nombre de chunks dÃ©croissant
        sorted_docs = sorted(doc_multiple_chunks.items(), key=lambda x: len(x[1]), reverse=True)
        
        for doc_id, chunks in sorted_docs[:10]:  # Afficher les 10 premiers
            print(f"ğŸ“„ Document: {doc_id[:12]}...")
            print(f"  Nombre de chunks: {len(chunks)}")
            
            # Afficher les mÃ©tadonnÃ©es du premier chunk
            first_chunk = chunks[0]
            print(f"  Titre: {first_chunk.metadata.get('title', 'N/A')}")
            print(f"  URL: {first_chunk.metadata.get('url', 'N/A')}")
            
            # VÃ©rifier la cohÃ©rence des mÃ©tadonnÃ©es
            metadata_consistent = True
            for chunk in chunks[1:]:
                if (chunk.metadata.get('title') != first_chunk.metadata.get('title') or
                    chunk.metadata.get('url') != first_chunk.metadata.get('url')):
                    metadata_consistent = False
                    break
            
            print(f"  MÃ©tadonnÃ©es cohÃ©rentes: {'âœ… OUI' if metadata_consistent else 'âŒ NON'}")
            print()
    else:
        print("âœ… Tous les documents ont un seul chunk")
    
    print()
    
    # === RÃ‰SUMÃ‰ ET RECOMMANDATIONS ===
    print("ğŸ“‹ RÃ‰SUMÃ‰ ET RECOMMANDATIONS")
    print("=" * 80)
    
    total_duplicate_chunks = sum(len(chunks) for chunks in id_duplicates.values())
    total_unique_chunks = len(docstore._dict) - total_duplicate_chunks + len(id_duplicates)
    
    print(f"ğŸ“Š Statistiques finales:")
    print(f"  - Chunks totaux: {len(docstore._dict)}")
    print(f"  - Chunks uniques: {total_unique_chunks}")
    print(f"  - Chunks dupliquÃ©s: {total_duplicate_chunks}")
    print(f"  - Taux de duplication: {(total_duplicate_chunks / len(docstore._dict)) * 100:.1f}%")
    
    if id_duplicates:
        print(f"\nğŸš¨ PROBLÃˆME MAJEUR: {len(id_duplicates)} IDs de chunks dupliquÃ©s!")
        print(f"   Cela explique pourquoi votre index FAISS a des chunks avec le mÃªme 'id'.")
        print(f"\nğŸ” Causes probables:")
        print(f"   1. Documents sources avec du contenu identique (erreurs 403, pages vides, etc.)")
        print(f"   2. Pipeline d'indexation exÃ©cutÃ© plusieurs fois sans nettoyage")
        print(f"   3. ProblÃ¨me dans le splitter qui gÃ©nÃ¨re des chunks identiques")
        print(f"\nğŸ› ï¸  Solutions:")
        print(f"   1. Nettoyer l'index FAISS existant")
        print(f"   2. Filtrer les documents sources avec un score de qualitÃ© trop bas")
        print(f"   3. Ajouter une vÃ©rification de doublons avant l'indexation")
        print(f"   4. Relancer l'indexation avec des sources propres")
    else:
        print(f"\nâœ… Aucun problÃ¨me de duplication d'ID dÃ©tectÃ©")

if __name__ == "__main__":
    analyze_faiss_duplicates()
